{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7012df-a3be-49dc-af50-29ec8382f4a0",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95ab2f-77e1-4c64-a3b1-cf963ba717db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from geocube.api.core import make_geocube\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = 'Times New Roman'\n",
    "\n",
    "# Define directories\n",
    "WORK_DIR = \"/beegfs/halder/SIMPLACE_WDIR/workshop/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57c92a-7eb2-448c-9992-ab50996482d4",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ff86e-2f64-40c0-9e59-d130112dcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data as a pandas dataframe\n",
    "data_path = os.path.join(WORK_DIR, \"data\", \"data.csv\")\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837fdcb",
   "metadata": {},
   "source": [
    "### Variables description\n",
    "\n",
    "\n",
    "| Variable                                         | Description                                                               | Unit / Notes          |\n",
    "| ------------------------------------------------ | ------------------------------------------------------------------------- | --------------------- |\n",
    "| **NUTS\\_ID**                                     | Regional identifier (NUTS3 level code for EU regions)                     | Categorical           |\n",
    "| **Harvest\\_Year**                                | Harvest year of the crop                                                  | Year (YYYY)           |\n",
    "| **Yield\\_t\\_ha**                                 | Simulated crop yield by Simplace                                          | tons per hectare      |\n",
    "| **Yield\\_Obs**                                   | Observed crop yield (reference/ground-truth)                              | tons per hectare      |\n",
    "| **crop**                                         | Crop type (e.g., maize, wheat)                                            | Categorical           |\n",
    "| **Rain\\_avg\\_pX**                                | Average rainfall during phenological stage *pX*                           | mm                    |\n",
    "| **Rain\\_1std\\_pX**                               | Count of days rainfall exceeded ±1σ deviation from mean during stage *pX* | count                 |\n",
    "| **Rain\\_2std\\_pX**                               | Count of days rainfall exceeded ±2σ deviation                             | count                 |\n",
    "| **Radiation\\_avg\\_pX**                           | Average solar radiation during stage *pX*                                 | MJ m⁻² day⁻¹          |\n",
    "| **Radiation\\_1std\\_pX**, **Radiation\\_2std\\_pX** | Extreme event counts for solar radiation                                  | count                 |\n",
    "| **Tmax\\_avg\\_pX**                                | Average maximum daily temperature during stage *pX*                       | °C                    |\n",
    "| **Tmax\\_1std\\_pX**, **Tmax\\_2std\\_pX**           | Extreme high/low temperature event counts                                 | count                 |\n",
    "| **Tmin\\_avg\\_pX**                                | Average minimum daily temperature during stage *pX*                       | °C                    |\n",
    "| **Tmin\\_1std\\_pX**, **Tmin\\_2std\\_pX**           | Extreme low temperature event counts                                      | count                 |\n",
    "| **ClimateWaterBalance\\_avg\\_pX**                 | Average rainfall – potential evapotranspiration (water availability)      | mm                    |\n",
    "| **TotalSoilAvailWaterContent\\_avg\\_pX**          | Mean soil water available for crops during stage *pX*                     | mm                    |\n",
    "| **TotalSoilAvailWaterContent\\_1std/2std\\_pX**    | Extreme soil water condition event counts                                 | count                 |\n",
    "| **LAI\\_max\\_pX**                                 | Maximum Leaf Area Index during stage *pX*                                 | m² leaf per m² ground |\n",
    "| **AGBiomass\\_t\\_ha\\_max\\_pX**                    | Maximum aboveground biomass during stage *pX*                             | tons per hectare      |\n",
    "| **ActualEvapotranspiration\\_max\\_pX**            | Maximum actual evapotranspiration during stage *pX*                       | mm                    |\n",
    "| **maxSeminal\\_RootDepth**                        | Maximum root depth attained                                               | cm                    |\n",
    "| **TRANRF**                                       | Ratio of actual to potential transpiration (water stress index)           | dimensionless         |\n",
    "| **NNI**                                          | Nitrogen Nutrition Index                                                  | dimensionless         |\n",
    "| **PNI**                                          | Phosphorus Nutrition Index                                                | dimensionless         |\n",
    "| **KNI**                                          | Potassium Nutrition Index                                                 | dimensionless         |\n",
    "| **NPKI**                                         | Combined NPK index (nutrient adequacy)                                    | dimensionless         |\n",
    "| **NUptake\\_SlimN**                               | Nitrogen uptake from soil mineral N pool                                  | kg N ha⁻¹             |\n",
    "| **cropNuptake\\_kg\\_ha**                          | Crop nitrogen uptake                                                      | kg N ha⁻¹             |\n",
    "| **cropPuptake\\_kg\\_ha**                          | Crop phosphorus uptake                                                    | kg P ha⁻¹             |\n",
    "| **TotalUptakeP**                                 | Total phosphorus uptake                                                   | kg P ha⁻¹             |\n",
    "| **CumulatedFertilizerP**                         | Cumulative phosphorus fertilizer input                                    | kg P ha⁻¹             |\n",
    "| **inputChemN\\_kg\\_ha**                           | Applied chemical nitrogen fertilizer                                      | kg N ha⁻¹             |\n",
    "| **PLeaching**                                    | Phosphorus lost via leaching                                              | kg P ha⁻¹             |\n",
    "| **CumulatedSeepN**                               | Total nitrogen lost by seepage                                            | kg N ha⁻¹             |\n",
    "| **NitrateLossSeepage**                           | Nitrate leached via seepage                                               | kg N ha⁻¹             |\n",
    "| **CumulatedMineralizedP**                        | Cumulative mineralized phosphorus from soil organic matter                | kg P ha⁻¹             |\n",
    "| **WSEEP\\_SUM**                                   | Water lost via seepage (cumulative)                                       | mm                    |\n",
    "| **soilwater\\_fc\\_global**                        | Soil water content at field capacity (global scale)                       | mm                    |\n",
    "| **soilwater\\_sat\\_global**                       | Soil water content at saturation (global scale)                           | mm                    |\n",
    "| **soilwater\\_fc\\_1**                             | Soil water at field capacity (topsoil layer)                              | mm                    |\n",
    "| **soilwater\\_sat\\_1–4**                          | Soil water at saturation for layers 1–4                                   | mm                    |\n",
    "| **carbon\\_1–5**                                  | Soil organic carbon content for depth layers                              | % or g C kg⁻¹ soil    |\n",
    "| **soil\\_quality\\_rating**                        | Soil quality index derived from soil properties                           | categorical / score   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac8198-d161-4c3c-b4e2-7fd6f6fb3a52",
   "metadata": {},
   "source": [
    "## Multicollinearity analysis\n",
    "This function helps identify and reduce **multicollinearity** in a dataset by detecting highly correlated features and retaining only a representative subset. This is especially useful before running machine learning models, where correlated features can inflate variance and reduce model interpretability.\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "* Detect **pairs of features** with high correlation.\n",
    "* Drop redundant features while keeping one representative feature from each correlated group.\n",
    "* Provide a reduced set of features that are less redundant and more informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a98ba8-dfd6-4148-90de-c8bae2e9891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify feature correlation\n",
    "def get_reduced_correlated_features(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Identifies correlated feature pairs and selects a reduced set of features\n",
    "    by keeping only one feature from each highly correlated group.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        threshold (float): Correlation coefficient threshold (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "        kept_features (List[str]): Features to keep (non-redundant).\n",
    "        dropped_features (List[str]): Features identified as redundant and dropped.\n",
    "        correlated_pairs (List[Tuple[str, str, float]]): Correlated feature pairs above threshold.\n",
    "    \"\"\"\n",
    "    corr_matrix = df.corr(numeric_only=True)\n",
    "    correlated_pairs = []\n",
    "    to_drop = set()\n",
    "    already_in_group = set()\n",
    "\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i + 1, len(corr_matrix.columns)):\n",
    "            col1 = corr_matrix.columns[i]\n",
    "            col2 = corr_matrix.columns[j]\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_value) >= threshold:\n",
    "                correlated_pairs.append((col1, col2, corr_value))\n",
    "                # Drop col2 if col1 hasn't already been marked to drop\n",
    "                if col1 not in to_drop and col2 not in already_in_group:\n",
    "                    to_drop.add(col2)\n",
    "                    already_in_group.add(col1)\n",
    "                    already_in_group.add(col2)\n",
    "\n",
    "    all_features = set(df.select_dtypes(include='number').columns)\n",
    "    kept_features = sorted(list(all_features - to_drop))\n",
    "    dropped_features = sorted(list(to_drop))\n",
    "\n",
    "    return kept_features, dropped_features, correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155aea7f-8e19-498e-976d-beb5982eee0b",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing sets by year\n",
    "This code splits the dataset into training and testing sets based on the number of unique `Harvest_Year` values. Instead of randomly splitting rows, it ensures the split happens chronologically (earlier years for training, later years for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0b4a0-9b77-499f-af47-13ad2b543baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of years\n",
    "n_train = int(data['Harvest_Year'].nunique() * 0.7)\n",
    "n_test = data['Harvest_Year'].nunique() - n_train\n",
    "\n",
    "train_years = sorted(data['Harvest_Year'].unique())[:n_train]\n",
    "test_years = sorted(data['Harvest_Year'].unique())[-n_test:]\n",
    "\n",
    "train_data = data[data['Harvest_Year'].isin(train_years)]\n",
    "test_data = data[data['Harvest_Year'].isin(test_years)]\n",
    "\n",
    "print(f'Train data shape: {train_data.shape}\\nTest data shape: {test_data.shape}')\n",
    "print('Years in the training data:', f'{min(train_years)}-{max(train_years)}')\n",
    "print('Years in the testing data:', f'{min(test_years)}-{max(test_years)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774fd37-9d85-4271-bd2e-558944885d47",
   "metadata": {},
   "source": [
    "## **Expanding window temporal cross validation**\n",
    "\n",
    "This function implements a **temporal cross-validation** approach where the model is trained on an **expanding window of past years** and tested on the following year(s).\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "1. Sort all unique years in the dataset.\n",
    "2. For each fold:\n",
    "\n",
    "   * **Training set**: all years from the earliest year up to a given year.\n",
    "   * **Testing set**: the next `test_years` immediately following the training period.\n",
    "3. Returns a list of **train/test index tuples** for each fold.\n",
    "\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "* `data` (`pd.DataFrame`): Dataset with a year column.\n",
    "* `year_col` (`str`): Column name representing the year.\n",
    "* `min_train_years` (`int`): Minimum years to start training.\n",
    "* `test_years` (`int`): Number of years to use for testing in each fold.\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `splits` (`list` of tuples): Each tuple contains `(train_indices, test_indices)` for one fold.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Keeps **temporal order** intact (no data leakage).\n",
    "* Simulates real-world forecasting: train on past, test on future.\n",
    "* Simple, easy-to-use for **time-series or seasonal data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5b88a-9aa4-48f5-8bf4-e7e06a5bc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_temporal_cv(\n",
    "    data,\n",
    "    year_col,\n",
    "    min_train_years=5,\n",
    "    test_years=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Expanding Window Temporal Cross-Validation\n",
    "\n",
    "    Splits the data into training and testing sets based on years using an expanding window.\n",
    "    - Training set: all years up to a given year\n",
    "    - Testing set: the immediately following `test_years`\n",
    "    \n",
    "    This is purely temporal CV; no locations are left out.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataframe containing a year column.\n",
    "    year_col : str\n",
    "        Column name representing the year (int or datetime).\n",
    "    min_train_years : int, default=5\n",
    "        Minimum number of years to begin training.\n",
    "    test_years : int, default=1\n",
    "        Number of years to use for testing in each fold.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    splits : list of (train_indices, test_indices)\n",
    "        List of train/test index tuples for each fold.\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    all_years = sorted(data[year_col].unique())\n",
    "\n",
    "    print(\"\\nExpanding Window Temporal Cross-Validation\\n\" + \"-\" * 60)\n",
    "\n",
    "    for end_train_idx in range(min_train_years, len(all_years) - test_years):\n",
    "        train_years = all_years[:end_train_idx]\n",
    "        test_years_range = all_years[end_train_idx:end_train_idx + test_years]\n",
    "\n",
    "        train_idx = data[data[year_col].isin(train_years)].index.tolist()\n",
    "        test_idx = data[data[year_col].isin(test_years_range)].index.tolist()\n",
    "\n",
    "        splits.append((train_idx, test_idx))\n",
    "        print(f\"Train: {train_years[0]}-{train_years[-1]} | Test: {test_years_range[0]}-{test_years_range[-1]} \"\n",
    "              f\"({len(train_idx)} train, {len(test_idx)} test)\")\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the group index\n",
    "group_index = expanding_window_temporal_cv(\n",
    "    train_data, \n",
    "    year_col='Harvest_Year',\n",
    "    min_train_years=19,\n",
    "    test_years=6\n",
    ")\n",
    "\n",
    "# Define the columns to be droppend from the data before training\n",
    "cols_to_be_dropped = [\"NUTS_ID\", \"Harvest_Year\", \"crop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07472d",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "This function standardizes the features of a training and testing dataset using the statistics from the training set only. This is a standard step in machine learning to make features comparable and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bb5ed-1fb0-4099-824e-eeb4e49fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize a train and test dataframe without considering NaN values\n",
    "def standardize_train_test(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Standardizes train and test DataFrames based on train data statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df (pd.DataFrame): Training dataset\n",
    "    - test_df (pd.DataFrame): Testing dataset\n",
    "    \n",
    "    Returns:\n",
    "    - standardized_train (pd.DataFrame): Standardized training data\n",
    "    - standardized_test (pd.DataFrame): Standardized test data\n",
    "    \"\"\"\n",
    "    mean = train_df.mean(skipna=True)\n",
    "    std = train_df.std(skipna=True)\n",
    "\n",
    "    standardized_train = (train_df - mean) / std\n",
    "    standardized_test = (test_df - mean) / std\n",
    "\n",
    "    return standardized_train, standardized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e4e74-ca42-415b-ba23-9f5990387b6d",
   "metadata": {},
   "source": [
    "## Model optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00103a-99b0-4cb5-ae82-fc86b3d3d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the highly correlated features from the data\n",
    "kept_features, dropped_features, correlated_pairs = get_reduced_correlated_features(\n",
    "    train_data.drop(columns=['NUTS_ID', 'Harvest_Year', 'crop', 'Yield_Obs']),\n",
    "    threshold=0.90\n",
    ")\n",
    "\n",
    "print('Number of highly correlated features:', len(dropped_features))\n",
    "\n",
    "# Drop the highly correlated features\n",
    "train_data = train_data.drop(columns=dropped_features)\n",
    "test_data = test_data.drop(columns=dropped_features)\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783ea5c",
   "metadata": {},
   "source": [
    "### Optuna Objective function for XGBoost hyperparameter tuning\n",
    "\n",
    "This function is used with **Optuna** to automatically search for the best hyperparameters of an `XGBRegressor` using **cross-validation**.\n",
    "\n",
    "1. **Define hyperparameter search space**\n",
    "\n",
    "   * `max_depth`: Maximum depth of each tree (shallow trees reduce overfitting).\n",
    "   * `learning_rate`: Step size shrinkage for boosting.\n",
    "   * `n_estimators`: Number of trees (boosting rounds).\n",
    "   * `min_child_weight`: Minimum sum of instance weight per leaf.\n",
    "   * `gamma`: Minimum loss reduction to make a split (regularization).\n",
    "   * `subsample`: Fraction of rows to sample per tree.\n",
    "   * `colsample_bytree`: Fraction of columns to sample per tree.\n",
    "   * `reg_alpha` & `reg_lambda`: L1 and L2 regularization terms.\n",
    "\n",
    "2. **Initialize XGBoost model**\n",
    "\n",
    "   * Using GPU (`device=\"cuda\"`) for faster training.\n",
    "   * `tree_method=\"hist\"` for optimized histogram-based training.\n",
    "\n",
    "3. **Cross-validation loop**\n",
    "\n",
    "   * Iterates over `group_index` containing predefined train/test splits (folds).\n",
    "   * For each fold:\n",
    "\n",
    "     1. Split train and test data.\n",
    "     2. Drop unnecessary columns and target column from features.\n",
    "     3. Standardize features using the `standardize_train_test` function.\n",
    "     4. Train the model on training data.\n",
    "     5. Predict on the test set.\n",
    "     6. Calculate **RMSE** (Root Mean Squared Error) for the fold.\n",
    "\n",
    "4. **Compute average RMSE**\n",
    "\n",
    "   * Average RMSE across all folds is returned as the **objective score** for Optuna to minimize.\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "* Automates hyperparameter tuning for XGBoost.\n",
    "* Uses cross-validation to avoid overfitting to a single train/test split.\n",
    "* Standardization ensures numerical stability across features.\n",
    "* GPU acceleration speeds up the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb24f2a-8b37-41e2-a753-dbfed99396b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for tuning XGBoost hyperparameters.\n",
    "    \n",
    "    Uses cross-validation to evaluate the average RMSE over predefined folds.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),                 # Tree depth\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.05),  # Learning rate\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),       # Number of trees\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 50), # Minimum sum of instance weight\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 0.5, 10),               # Regularization term for splits\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.8),         # Row subsampling\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 0.8), # Column subsampling\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1, 50),         # L1 regularization\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1, 50),       # L2 regularization\n",
    "    }\n",
    "\n",
    "    # Initialize the XGBoost model\n",
    "    model = XGBRegressor(\n",
    "        **params, \n",
    "        tree_method=\"hist\", \n",
    "        device=\"cuda\", \n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    fold_losses = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for _, (train_idx, test_idx) in enumerate(group_index):\n",
    "        # Split features and target\n",
    "        X_train = train_data.loc[train_idx].drop(cols_to_be_dropped + ['Yield_Obs'], axis=1)\n",
    "        y_train = train_data.loc[train_idx]['Yield_Obs']\n",
    "\n",
    "        X_test = train_data.loc[test_idx].drop(cols_to_be_dropped + ['Yield_Obs'], axis=1)\n",
    "        y_test = train_data.loc[test_idx]['Yield_Obs']\n",
    "\n",
    "        # Standardize the train and test sets\n",
    "        X_train_scaled, X_test_scaled = standardize_train_test(X_train, X_test)\n",
    "\n",
    "        # Fit the model and predict\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Compute RMSE for the fold\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_losses.append(fold_rmse)\n",
    "\n",
    "    # Return average RMSE across folds\n",
    "    return np.mean(fold_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb19a49-e6f5-48b7-b26c-f55ea08c0466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"xgboost\", sampler=sampler, direction=\"minimize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c191f",
   "metadata": {},
   "source": [
    "## Fitting the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ce661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train the final model\n",
    "X_train_info = train_data[cols_to_be_dropped + ['Yield_Obs']]\n",
    "X_train = train_data.drop(cols_to_be_dropped + ['Yield_Obs'], axis=1)\n",
    "y_train = train_data['Yield_Obs']\n",
    "\n",
    "X_test_info = test_data[cols_to_be_dropped + ['Yield_Obs']]\n",
    "X_test = test_data.drop(cols_to_be_dropped + ['Yield_Obs'], axis=1)\n",
    "y_test = test_data['Yield_Obs']\n",
    "\n",
    "# Standardize the data\n",
    "X_train_scaled, X_test_scaled = standardize_train_test(X_train, X_test)\n",
    "\n",
    "# Create a model instance\n",
    "model = XGBRegressor(**best_params, tree_method=\"hist\", predictor=\"gpu_predictor\", device=\"cuda\", random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and train set\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "# Update the preds_df\n",
    "test_preds = X_test_info.copy()\n",
    "train_preds = X_train_info.copy()\n",
    "\n",
    "test_preds['Yield_Preds'] = y_pred_test\n",
    "train_preds['Yield_Preds'] = y_pred_train\n",
    "\n",
    "print(train_preds.shape, test_preds.shape)\n",
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c801ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the predicted data from both the scenario\n",
    "pred_hybrid = test_preds.copy().drop('crop', axis=1)\n",
    "pred_hybrid.rename(columns={'Yield_Obs': 'Yield (Obs)', 'Yield_Preds': 'Yield (Hybrid)'}, inplace=True)\n",
    "\n",
    "# Add the result from the Simulated yield\n",
    "sim_yield_df = data[['NUTS_ID', 'Harvest_Year', 'Yield_t_ha']].groupby(by=['NUTS_ID', 'Harvest_Year']).mean().reset_index()\n",
    "sim_yield_df.rename(columns={'Yield_t_ha': 'Yield (PBM)'}, inplace=True)\n",
    "\n",
    "global_preds = pd.merge(left=sim_yield_df, right=pred_hybrid,\n",
    "                        on=['NUTS_ID', 'Harvest_Year'], how='inner')\n",
    "\n",
    "global_preds = global_preds[['NUTS_ID', 'Harvest_Year', 'Yield (Obs)', 'Yield (PBM)', 'Yield (Hybrid)']]\n",
    "print(global_preds.shape)\n",
    "global_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509903bd-154f-4505-8388-e0bdb5ac085c",
   "metadata": {},
   "source": [
    "## Upscaling the model from NUTS3 to pixel scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48831e-60e3-4685-9d5a-f12d3b8bf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datapath\n",
    "data_path = os.path.join(WORK_DIR, 'data', \"data_1km.csv\")\n",
    "data_1km = pd.read_csv(data_path)\n",
    "\n",
    "data_1km_info = data_1km[['Location', 'NUTS_ID', 'Harvest_Year', 'Yield_t_ha', 'geometry']]\n",
    "data_1km_info.rename(columns={'Yield_t_ha': 'Yield (PBM)'}, inplace=True)\n",
    "\n",
    "# Drop all the correlated and unnecessary features\n",
    "data_1km = data_1km[X_train_scaled.columns]\n",
    "\n",
    "print(data_1km.shape)\n",
    "data_1km.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53764f85-e770-4736-962a-acd952ebe5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized the data\n",
    "_, data_1km_scaled = standardize_train_test(X_train, data_1km)\n",
    "\n",
    "# Predict the yield\n",
    "data_1km_info['Yield (Hybrid-1sqkm)'] = model.predict(data_1km_scaled)\n",
    "print(data_1km_info.shape)\n",
    "data_1km_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2c5ef-8ffd-4064-949b-2448623f6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the data and compare with observed yield\n",
    "data_1km_grouped = data_1km_info.drop(columns=['Location', 'geometry'])\\\n",
    "    .groupby(by=['NUTS_ID', 'Harvest_Year'])\\\n",
    "    .mean()\\\n",
    "    .reset_index()\n",
    "    \n",
    "data_1km_grouped = pd.merge(\n",
    "    left=data_1km_grouped, \n",
    "    right=global_preds[['NUTS_ID', 'Harvest_Year', 'Yield (Obs)']],\n",
    "    how='inner',\n",
    "    on=['NUTS_ID', 'Harvest_Year']\n",
    ")\n",
    "\n",
    "print(data_1km_grouped.shape)\n",
    "data_1km_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf1a49-da2a-4271-9e11-36a4f93286c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_preds = pd.merge(\n",
    "    left=global_preds, \n",
    "    right=data_1km_grouped[['NUTS_ID', 'Harvest_Year', 'Yield (Hybrid-1sqkm)']],\n",
    "    on=['NUTS_ID', 'Harvest_Year'],\n",
    "    how='inner'\n",
    ")\n",
    "print(global_preds.shape)\n",
    "global_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334de597-58c6-40a9-9d24-15f19ec476a7",
   "metadata": {},
   "source": [
    "## Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c3c3c-00a7-4a50-9912-985c8ef6f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt to long format for ease of plotting\n",
    "df_melted = global_preds.melt(id_vars=[\"NUTS_ID\", \"Harvest_Year\", \"Yield (Obs)\"],\n",
    "                    value_vars=['Yield (PBM)', 'Yield (Hybrid)', 'Yield (Hybrid-1sqkm)'],\n",
    "                    var_name=\"Type\", value_name=\"Predicted\")\n",
    "\n",
    "# Calculate MAPE\n",
    "df_melted[\"MAPE\"] = np.abs((df_melted[\"Yield (Obs)\"] - df_melted[\"Predicted\"]) / df_melted[\"Yield (Obs)\"]) * 100\n",
    "print(df_melted.shape)\n",
    "df_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57aaea-be60-43c5-af72-94a2fc05b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean MAPE per model type\n",
    "mean_mapes = df_melted.groupby(\"Type\")[\"MAPE\"].mean()\n",
    "\n",
    "# Define colors to match barplot\n",
    "colors = {\n",
    "    \"Yield (PBM)\": \"#ffa600\",\n",
    "    \"Yield (Hybrid)\": \"#ff6361\",\n",
    "    \"Yield (Hybrid-1sqkm)\": \"#58508d\"\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_melted,\n",
    "    x=\"Harvest_Year\", y=\"MAPE\", hue=\"Type\",\n",
    "    ci=\"sd\", capsize=0.2, errwidth=1.2,\n",
    "    palette=[colors[\"Yield (PBM)\"], colors[\"Yield (Hybrid)\"], colors[\"Yield (Hybrid-1sqkm)\"]],\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Add horizontal mean lines\n",
    "for model_type, mean_val in mean_mapes.items():\n",
    "    plt.axhline(y=mean_val, color=colors[model_type], linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Save handles and labels from legend\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# Custom label mapping\n",
    "label_map = {\n",
    "    \"Yield (PBM)\": f\"PBM ({mean_mapes['Yield (PBM)'].round(2)}%)\",\n",
    "    \"Yield (Hybrid)\": f\"Hybrid ({mean_mapes['Yield (Hybrid)'].round(2)}%)\",\n",
    "    \"Yield (Hybrid-1sqkm)\": f\"Hybrid (1 km²) ({mean_mapes['Yield (Hybrid-1sqkm)'].round(2)}%)\"\n",
    "}\n",
    "\n",
    "# Apply new labels to legend\n",
    "plt.legend(handles=handles, labels=[label_map[l] for l in labels], title=\"\", loc='upper left', frameon=False)\n",
    "\n",
    "# Final touches\n",
    "plt.title(\"Year-wise MAPE of Yield Predictions (with Standard Deviation)\")\n",
    "plt.xlabel(\"Test Year\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c93d8-1612-4379-a280-d4aae1b1c00a",
   "metadata": {},
   "source": [
    "## Convert the prediction Into a raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae08fed-2792-4f3c-9a37-2f92cca36d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Berlin', 'Brandenburg']\n",
    "\n",
    "# Read the coordinates and convert it into geodataframe\n",
    "coords_df = pd.read_csv(os.path.join(WORK_DIR, 'data', 'location.csv'))\n",
    "coords_df[\"geometry\"] = coords_df[\"geometry\"].apply(wkt.loads)\n",
    "coords_gdf = gpd.GeoDataFrame(coords_df, geometry='geometry', crs='EPSG:3857')\n",
    "coords_gdf.to_crs('EPSG:31467', inplace=True)\n",
    "coords_gdf = coords_gdf[coords_gdf['STATE_NAME'].isin(states)]\n",
    "coords_gdf.rename(columns={'Cell_ID': 'Location'}, inplace=True)\n",
    "print(coords_gdf.shape)\n",
    "coords_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6c614-a9ef-4bc4-8934-777071f22ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1km_info_gdf = pd.merge(\n",
    "    left=data_1km_info.drop(columns=['geometry']), \n",
    "    right=coords_gdf[['Location', 'geometry']],\n",
    "    on=['Location'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "data_1km_info_gdf['geometry'] = data_1km_info_gdf['geometry'].apply(\n",
    "    lambda x: wkt.loads(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "data_1km_info_gdf = gpd.GeoDataFrame(data_1km_info_gdf, crs='EPSG:31467')\n",
    "data_1km_info_gdf.rename(columns={\"Yield (Hybrid-1sqkm)\": 'Yield (Hybrid)'}, inplace=True)\n",
    "print(data_1km_info_gdf.shape)\n",
    "data_1km_info_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443bace-953c-4b16-8ba8-5ada45f2dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years = sorted(data_1km_info_gdf[\"Harvest_Year\"].unique())\n",
    "\n",
    "# Dictionary to hold lists of DataArrays for each measurement\n",
    "measurements = list(data_1km_info_gdf.columns[3:-1])\n",
    "measurement_arrays = {m: list() for m in measurements}\n",
    "\n",
    "# Loop through each year and rasterize\n",
    "for year in tqdm(unique_years):\n",
    "    gdf_year = data_1km_info_gdf[data_1km_info_gdf[\"Harvest_Year\"] == year]\n",
    "\n",
    "    cube = make_geocube(\n",
    "        vector_data=gdf_year,\n",
    "        measurements=measurements,\n",
    "        resolution=(-1000, 1000),\n",
    "        fill=np.nan\n",
    "    )\n",
    "\n",
    "    for measurement in measurement_arrays:\n",
    "        # Expand along time dimension and add to list\n",
    "        data_array = cube[measurement].expand_dims(year=[year])\n",
    "        measurement_arrays[measurement].append(data_array)\n",
    "\n",
    "# Combine each measurement into one DataArray (over years)\n",
    "combined_vars = {\n",
    "    var_name: xr.concat(arr_list, dim=\"year\")\n",
    "    for var_name, arr_list in measurement_arrays.items()\n",
    "}\n",
    "\n",
    "# Create a Dataset from all variables\n",
    "combined_dataset = xr.Dataset(combined_vars)\n",
    "\n",
    "# Save to NetCDF\n",
    "combined_dataset.to_netcdf(os.path.join(WORK_DIR, 'output', 'prediction.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e06d3-cd5d-4c67-8641-c8d3b3451065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = xr.open_dataset(os.path.join(WORK_DIR, 'output', 'prediction.nc'))\n",
    "\n",
    "var_name = \"Yield (Hybrid)\"\n",
    "\n",
    "# Compute mean and std over spatial dims (x, y) for each year\n",
    "mean_vals = ds[var_name].mean(dim=[\"x\", \"y\"])\n",
    "std_vals = ds[var_name].std(dim=[\"x\", \"y\"])\n",
    "\n",
    "years = mean_vals.year.values\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(years, mean_vals, color='blue', marker='o', label=var_name)\n",
    "plt.fill_between(years,\n",
    "                 mean_vals - std_vals,\n",
    "                 mean_vals + std_vals,\n",
    "                 color='blue',\n",
    "                 alpha=0.1)\n",
    "\n",
    "plt.title(f\"Mean {var_name} over time with standard deviation\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(f\"{var_name}\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cea2a-6094-4e60-839e-655e8d1c0576",
   "metadata": {},
   "source": [
    "## Apply the model on CMIP future scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd70b3-2f57-48da-9d71-3c5d85af32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scenario name\n",
    "scenario_name = 'MPI-ESM1-2-HR_ssp585'\n",
    "\n",
    "# Define the datapath\n",
    "data_path = os.path.join(WORK_DIR, 'data', f\"data_1km_{scenario_name}.csv\")\n",
    "scenario_data = pd.read_csv(data_path)\n",
    "scenario_data = scenario_data[scenario_data['Harvest_Year']>2022].reset_index(drop=True)\n",
    "future = scenario_data[X_train.columns]\n",
    "scenario_data_info = scenario_data[['Location', 'NUTS_ID', 'Harvest_Year', 'Yield_t_ha']]\n",
    "scenario_data_info.rename(columns={'Yield_t_ha': 'Yield (PBM)'}, inplace=True)\n",
    "\n",
    "X_train_scaled, future_scaled = standardize_train_test(X_train, future)\n",
    "\n",
    "# Predict the yield\n",
    "scenario_data_info ['Yield (Hybrid)'] = model.predict(future_scaled)\n",
    "\n",
    "scenario_data_info = pd.merge(\n",
    "    left=scenario_data_info,\n",
    "    right=data_1km_info_gdf[['Location', 'geometry']].drop_duplicates(subset='Location'),\n",
    "    on=['Location'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "scenario_data_info = gpd.GeoDataFrame(scenario_data_info)\n",
    "print(scenario_data_info.shape)\n",
    "scenario_data_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948ed25-e379-48ed-a6fc-c0242139fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years = sorted(scenario_data_info[\"Harvest_Year\"].unique())\n",
    "\n",
    "# Dictionary to hold lists of DataArrays for each measurement\n",
    "measurements = list(data_1km_info_gdf.columns[3:-1])\n",
    "measurement_arrays = {m: list() for m in measurements}\n",
    "\n",
    "# Loop through each year and rasterize\n",
    "for year in tqdm(unique_years):\n",
    "    gdf_year = scenario_data_info[scenario_data_info[\"Harvest_Year\"] == year]\n",
    "\n",
    "    cube = make_geocube(\n",
    "        vector_data=gdf_year,\n",
    "        measurements=measurements,\n",
    "        resolution=(-1000, 1000),\n",
    "        fill=np.nan\n",
    "    )\n",
    "\n",
    "    for measurement in measurement_arrays:\n",
    "        # Expand along time dimension and add to list\n",
    "        data_array = cube[measurement].expand_dims(year=[year])\n",
    "        measurement_arrays[measurement].append(data_array)\n",
    "\n",
    "# Combine each measurement into one DataArray (over years)\n",
    "combined_vars = {\n",
    "    var_name: xr.concat(arr_list, dim=\"year\")\n",
    "    for var_name, arr_list in measurement_arrays.items()\n",
    "}\n",
    "\n",
    "# Create a Dataset from all variables\n",
    "combined_dataset = xr.Dataset(combined_vars)\n",
    "\n",
    "# Save to NetCDF\n",
    "combined_dataset.to_netcdf(os.path.join(WORK_DIR, 'output', f\"prediction_{scenario_name}.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80d3b6-f141-49bd-8bce-61b63b3c3496",
   "metadata": {},
   "source": [
    "## Final plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e102b6-c133-4551-979d-9705619c6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare NetCDF datasets\n",
    "hist_ds = xr.open_dataset(os.path.join(WORK_DIR, 'output', 'prediction.nc'))\n",
    "futu_ds = xr.open_dataset(os.path.join(WORK_DIR, 'output', f'prediction_{scenario_name}.nc'))\n",
    "\n",
    "var_name = \"Yield (PBM)\"\n",
    "hist_var = hist_ds[var_name]\n",
    "futu_var = futu_ds[var_name]\n",
    "\n",
    "# Assign CRS and reproject both datasets to EPSG:4326\n",
    "hist_var = hist_var.rio.write_crs(\"EPSG:31467\").rio.reproject(\"EPSG:4326\")\n",
    "futu_var = futu_var.rio.write_crs(\"EPSG:31467\").rio.reproject(\"EPSG:4326\")\n",
    "\n",
    "# Concatenate datasets along 'year'\n",
    "combined_var = xr.concat([hist_var, futu_var], dim='year')\n",
    "combined_var = combined_var.sortby('year')\n",
    "\n",
    "# Define 15-year intervals\n",
    "intervals = [(1980, 2000), (2001, 2020), (2021, 2040), (2041, 2060), (2061, 2080), (2081, 2100)]\n",
    "\n",
    "# Load shapefile (Brandenburg)\n",
    "gdf = gpd.read_file(os.path.join(WORK_DIR, 'data', 'brandenburg.gpkg'))\n",
    "\n",
    "# Compute 15-year means\n",
    "mean_maps = []\n",
    "for start, end in intervals:\n",
    "    mean = combined_var.sel(year=slice(start, end)).mean(dim='year')\n",
    "    mean_maps.append((f\"{start}-{end}\", mean))\n",
    "\n",
    "# Determine common color scale\n",
    "vmin = min([m[1].min().item() for m in mean_maps])\n",
    "vmax = max([m[1].max().item() for m in mean_maps])\n",
    "\n",
    "# Plot subplots\n",
    "n_maps = len(mean_maps)\n",
    "ncols = 3\n",
    "nrows = (n_maps + ncols - 1) // ncols\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows),\n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot subplots\n",
    "for i, (title, d) in enumerate(mean_maps):\n",
    "    ax = axes[i]\n",
    "    gdf.boundary.plot(ax=ax, edgecolor='black', linewidth=0.8, transform=ccrs.PlateCarree(), zorder=3)\n",
    "    im = d.plot(ax=ax, transform=ccrs.PlateCarree(), cmap='RdYlGn',\n",
    "                   vmin=vmin, vmax=vmax, add_colorbar=False, zorder=2)\n",
    "    # gdf.boundary.plot(ax=ax, edgecolor='black', facecolor='#eaeaea', linewidth=0.8, transform=ccrs.PlateCarree(), zorder=1)\n",
    "    ax.set_title(f\"Mean Yield: {float(d.mean()):.2f} | Year: {title}\", fontsize=14)\n",
    "\n",
    "# Remove unused axes if any\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add shared colorbar on the right side\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, label=f'{var_name} (t/ha)')\n",
    "cbar.set_label(f'{var_name} (t/ha)', fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=12) \n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d4ac8-a1b2-4764-9504-db7369444ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Yield (PBM)', 'Yield (Hybrid)']\n",
    "yearly_timeseries = {}\n",
    "\n",
    "for var in var_names:\n",
    "    hist_var = hist_ds[var]\n",
    "    futu_var = futu_ds[var]\n",
    "    \n",
    "    combined_var = xr.concat([hist_var, futu_var], dim='year')\n",
    "    combined_var = combined_var.sortby('year')\n",
    "    yearly_mean = combined_var.mean(dim=['y', 'x'])\n",
    "    yearly_std = combined_var.std(dim=['y', 'x'])\n",
    "    yearly_df = pd.DataFrame(\n",
    "        {'Year': yearly_mean.year, 'Yield (Mean)': yearly_mean, 'Yield (Std)': yearly_std}\n",
    "    )\n",
    "    yearly_timeseries[var] = yearly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e063aa6-4dde-4096-bdf9-44530fc2a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4), dpi=150)\n",
    "for i, (t, df) in enumerate(yearly_timeseries.items()):\n",
    "    sns.lineplot(data=df, x='Year', y='Yield (Mean)', marker='o', markersize=4, label=t)\n",
    "    \n",
    "    # Plot the fill between mean ± std\n",
    "    plt.fill_between(\n",
    "        df['Year'],\n",
    "        df['Yield (Mean)'] - df['Yield (Std)'],\n",
    "        df['Yield (Mean)'] + df['Yield (Std)'],\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "sns.lineplot(data=data, x='Harvest_Year', y='Yield_Obs', marker='o', markersize=4, label='Yield (Observed)') \n",
    "plt.grid()\n",
    "plt.ylabel('Yield (t/ha)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c35f6-bc8a-4e24-8322-885ebca6e327",
   "metadata": {},
   "source": [
    "## **SHAP analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fcc0f-3f76-4ead-98b9-98ed1e90b8e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for the entire test set\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# summary plot (global importance)\n",
    "shap.summary_plot(shap_values, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c823b-8fc3-41e1-80da-4bbbd0abe508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and names\n",
    "selected_variables = {\n",
    "    'Minimum Temperature (Pre-planting window)': 'Tmin_avg_p0',\n",
    "    'Radiation (Vegetative phase)': 'Radiation_avg_p2',\n",
    "    'Minimum Temperature (Harvest window)': 'Tmin_avg_p5',\n",
    "    'Precipitation (Planting window)': 'Rain_avg_p1',\n",
    "    'Above Ground Biomass (Vegetative phase)': 'AGBiomass_t_ha_max_p2',\n",
    "    'Total Soil-Available Water (Vegetative phase)': 'TotalSoilAvailWaterContent_avg_p2'\n",
    "}\n",
    "\n",
    "# Set up plot grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Set consistent color normalization range across all plots\n",
    "shap_min, shap_max = shap_values.min(), shap_values.max()\n",
    "\n",
    "# Loop through variables\n",
    "for i, (name, var) in enumerate(selected_variables.items()):\n",
    "    shap_vals = shap_values[:, X_test.columns.get_loc(var)]\n",
    "    feat_vals = X_test[var]\n",
    "    \n",
    "    # DataFrame for plotting\n",
    "    df = pd.DataFrame({var: feat_vals, 'SHAP': shap_vals})\n",
    "    \n",
    "    # LOWESS smoothing\n",
    "    smoothed = lowess(endog=df['SHAP'], exog=df[var], frac=0.2)\n",
    "    \n",
    "    # Scatter plot with SHAP coloring\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=var,\n",
    "        y='SHAP',\n",
    "        hue='SHAP',\n",
    "        palette='PiYG',\n",
    "        hue_norm=(shap_min, shap_max),\n",
    "        ax=axes[i],\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "        edgecolor='k',\n",
    "        linewidth=0.5,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    # Smoothed trend line\n",
    "    axes[i].plot(smoothed[:, 0], smoothed[:, 1], color='red', lw=2, label='Smoothed trend')\n",
    "    \n",
    "    # Horizontal zero line\n",
    "    axes[i].axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Labels\n",
    "    axes[i].set_xlabel(name, fontsize=11)\n",
    "    axes[i].set_ylabel('SHAP value', fontsize=11)\n",
    "    axes[i].tick_params(labelsize=10)\n",
    "    axes[i].set_title(name, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Layout and title\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"SHAP Dependency Plots for Key Predictors\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
